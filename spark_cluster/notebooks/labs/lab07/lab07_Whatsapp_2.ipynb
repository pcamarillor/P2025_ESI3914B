{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Ejemplos de Spark: Structured Streaming (Files)** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/02 17:33:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"tarea-Files1\") \\\n",
    "    .master(\"spark://fc91669459e3:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whatsapp2.spark_utils import SparkUtils\n",
    "from whatsapp2.logs_query_listener import LogsListener\n",
    "\n",
    "spark.streams.addListener(LogsListener())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del FileStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_name.spark_utils import SparkUtils\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "\n",
    "logs_df = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"text\") \\\n",
    "                .option(\"maxFilesPerTrigger\", 1) \\\n",
    "                .load(\"/home/jovyan/notebooks/data/structured_streaming_files/logs/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- log: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- server: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structured_logs_df = logs_df.select(split(logs_df.value, \" \\\\| \").alias(\"log\"))\n",
    "structured_logs_df = structured_logs_df.withColumn(\"timestamp\", structured_logs_df[\"log\"].getItem(0).cast(\"timestamp\"))\n",
    "structured_logs_df = structured_logs_df.withColumn(\"level\", structured_logs_df[\"log\"].getItem(1).cast(\"string\"))\n",
    "structured_logs_df = structured_logs_df.withColumn(\"message\", structured_logs_df[\"log\"].getItem(2).cast(\"string\"))\n",
    "structured_logs_df = structured_logs_df.withColumn(\"server\", structured_logs_df[\"log\"].getItem(3).cast(\"string\"))\n",
    "\n",
    "structured_logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_logs_df = structured_logs_df.filter(structured_logs_df[\"level\"] == \"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del \"Sink\" del stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/02 17:35:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f9ed1e9c-73e4-4221-9857-0540f8c64b19. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/04/02 17:35:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started: 1ce9aab7-f610-4f85-b7fe-ce08ff5d96c1\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-----+--------------------+-------------+\n",
      "|                 log|          timestamp|level|             message|       server|\n",
      "+--------------------+-------------------+-----+--------------------+-------------+\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:20|ERROR|       403 Forbidden|server-node-2|\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:21|ERROR|     502 Bad Gateway|server-node-3|\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:22|ERROR|500 Internal Serv...|server-node-1|\n",
      "+--------------------+-------------------+-----+--------------------+-------------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"1ce9aab7-f610-4f85-b7fe-ce08ff5d96c1\",\n",
      "  \"runId\" : \"2dafe564-a7c6-499f-bfa7-356846739c45\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-02T17:35:17.647Z\",\n",
      "  \"batchId\" : 0,\n",
      "  \"numInputRows\" : 5,\n",
      "  \"inputRowsPerSecond\" : 0.0,\n",
      "  \"processedRowsPerSecond\" : 1.9607843137254903,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 1150,\n",
      "    \"commitOffsets\" : 32,\n",
      "    \"getBatch\" : 155,\n",
      "    \"latestOffset\" : 936,\n",
      "    \"queryPlanning\" : 41,\n",
      "    \"triggerExecution\" : 2550,\n",
      "    \"walCommit\" : 228\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/structured_streaming_files/logs]\",\n",
      "    \"startOffset\" : null,\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 0\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 5,\n",
      "    \"inputRowsPerSecond\" : 0.0,\n",
      "    \"processedRowsPerSecond\" : 1.9607843137254903\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3b4b7fb5\",\n",
      "    \"numOutputRows\" : 3\n",
      "  }\n",
      "}\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---+---------+-----+-------+------+\n",
      "|log|timestamp|level|message|server|\n",
      "+---+---------+-----+-------+------+\n",
      "+---+---------+-----+-------+------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-----+---------------+-------------+\n",
      "|                 log|          timestamp|level|        message|       server|\n",
      "+--------------------+-------------------+-----+---------------+-------------+\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:53|ERROR|  403 Forbidden|server-node-2|\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:56|ERROR|502 Bad Gateway|server-node-3|\n",
      "+--------------------+-------------------+-----+---------------+-------------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"1ce9aab7-f610-4f85-b7fe-ce08ff5d96c1\",\n",
      "  \"runId\" : \"2dafe564-a7c6-499f-bfa7-356846739c45\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-02T17:35:20.199Z\",\n",
      "  \"batchId\" : 1,\n",
      "  \"numInputRows\" : 5,\n",
      "  \"inputRowsPerSecond\" : 1.9592476489028212,\n",
      "  \"processedRowsPerSecond\" : 3.5486160397444997,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 1199,\n",
      "    \"commitOffsets\" : 41,\n",
      "    \"getBatch\" : 62,\n",
      "    \"latestOffset\" : 30,\n",
      "    \"queryPlanning\" : 32,\n",
      "    \"triggerExecution\" : 1409,\n",
      "    \"walCommit\" : 41\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/structured_streaming_files/logs]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 0\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 1\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 5,\n",
      "    \"inputRowsPerSecond\" : 1.9592476489028212,\n",
      "    \"processedRowsPerSecond\" : 3.5486160397444997\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3b4b7fb5\",\n",
      "    \"numOutputRows\" : 0\n",
      "  }\n",
      "}\n",
      "Query made progress: {\n",
      "  \"id\" : \"9b58389b-0b9e-46ad-8ae3-08de427e8188\",\n",
      "  \"runId\" : \"fb597df6-a86d-4639-a419-eb22ee826f40\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-02T17:35:20.001Z\",\n",
      "  \"batchId\" : 6,\n",
      "  \"numInputRows\" : 5,\n",
      "  \"inputRowsPerSecond\" : 0.4999500049995001,\n",
      "  \"processedRowsPerSecond\" : 2.880184331797235,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 405,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 628,\n",
      "    \"latestOffset\" : 357,\n",
      "    \"queryPlanning\" : 269,\n",
      "    \"triggerExecution\" : 1735,\n",
      "    \"walCommit\" : 34\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/structured_streaming_files/logs]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 5\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 6\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 5,\n",
      "    \"inputRowsPerSecond\" : 0.4999500049995001,\n",
      "    \"processedRowsPerSecond\" : 2.880184331797235\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3b4b7fb5\",\n",
      "    \"numOutputRows\" : 2\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-----+---------------+-------------+\n",
      "|                 log|          timestamp|level|        message|       server|\n",
      "+--------------------+-------------------+-----+---------------+-------------+\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:57|ERROR|502 Bad Gateway|server-node-1|\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:58|ERROR|  403 Forbidden|server-node-1|\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:59|ERROR|  403 Forbidden|server-node-1|\n",
      "|[2025-04-02 17:35...|2025-04-02 17:35:00|ERROR|  404 Not Found|server-node-1|\n",
      "+--------------------+-------------------+-----+---------------+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "|                 log|          timestamp|level|      message|       server|\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:31|ERROR|404 Not Found|server-node-2|\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"9b58389b-0b9e-46ad-8ae3-08de427e8188\",\n",
      "  \"runId\" : \"fb597df6-a86d-4639-a419-eb22ee826f40\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-02T17:35:30.001Z\",\n",
      "  \"batchId\" : 7,\n",
      "  \"numInputRows\" : 5,\n",
      "  \"inputRowsPerSecond\" : 0.5,\n",
      "  \"processedRowsPerSecond\" : 3.6179450072358903,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 1231,\n",
      "    \"commitOffsets\" : 39,\n",
      "    \"getBatch\" : 38,\n",
      "    \"latestOffset\" : 26,\n",
      "    \"queryPlanning\" : 18,\n",
      "    \"triggerExecution\" : 1382,\n",
      "    \"walCommit\" : 25\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/structured_streaming_files/logs]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 6\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 7\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 5,\n",
      "    \"inputRowsPerSecond\" : 0.5,\n",
      "    \"processedRowsPerSecond\" : 3.6179450072358903\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3b4b7fb5\",\n",
      "    \"numOutputRows\" : 4\n",
      "  }\n",
      "}\n",
      "Query made progress: {\n",
      "  \"id\" : \"1ce9aab7-f610-4f85-b7fe-ce08ff5d96c1\",\n",
      "  \"runId\" : \"2dafe564-a7c6-499f-bfa7-356846739c45\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-02T17:35:30.001Z\",\n",
      "  \"batchId\" : 2,\n",
      "  \"numInputRows\" : 5,\n",
      "  \"inputRowsPerSecond\" : 0.5100999795960008,\n",
      "  \"processedRowsPerSecond\" : 2.371916508538899,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 1672,\n",
      "    \"commitOffsets\" : 324,\n",
      "    \"getBatch\" : 38,\n",
      "    \"latestOffset\" : 26,\n",
      "    \"queryPlanning\" : 18,\n",
      "    \"triggerExecution\" : 2108,\n",
      "    \"walCommit\" : 26\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/structured_streaming_files/logs]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 1\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 2\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 5,\n",
      "    \"inputRowsPerSecond\" : 0.5100999795960008,\n",
      "    \"processedRowsPerSecond\" : 2.371916508538899\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3b4b7fb5\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-----+--------------------+-------------+\n",
      "|                 log|          timestamp|level|             message|       server|\n",
      "+--------------------+-------------------+-----+--------------------+-------------+\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:36|ERROR|       404 Not Found|server-node-1|\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:39|ERROR|500 Internal Serv...|server-node-2|\n",
      "+--------------------+-------------------+-----+--------------------+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-----+---------------+-------------+\n",
      "|                 log|          timestamp|level|        message|       server|\n",
      "+--------------------+-------------------+-----+---------------+-------------+\n",
      "|[2025-04-02 17:35...|2025-04-02 17:35:05|ERROR|  403 Forbidden|server-node-1|\n",
      "|[2025-04-02 17:35...|2025-04-02 17:35:06|ERROR|502 Bad Gateway|server-node-1|\n",
      "+--------------------+-------------------+-----+---------------+-------------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"1ce9aab7-f610-4f85-b7fe-ce08ff5d96c1\",\n",
      "  \"runId\" : \"2dafe564-a7c6-499f-bfa7-356846739c45\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-02T17:35:40.001Z\",\n",
      "  \"batchId\" : 3,\n",
      "  \"numInputRows\" : 5,\n",
      "  \"inputRowsPerSecond\" : 0.5,\n",
      "  \"processedRowsPerSecond\" : 13.404825737265416,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 218,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 43,\n",
      "    \"latestOffset\" : 36,\n",
      "    \"queryPlanning\" : 11,\n",
      "    \"triggerExecution\" : 373,\n",
      "    \"walCommit\" : 33\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/structured_streaming_files/logs]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 2\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 3\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 5,\n",
      "    \"inputRowsPerSecond\" : 0.5,\n",
      "    \"processedRowsPerSecond\" : 13.404825737265416\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3b4b7fb5\",\n",
      "    \"numOutputRows\" : 2\n",
      "  }\n",
      "}\n",
      "Query made progress: {\n",
      "  \"id\" : \"9b58389b-0b9e-46ad-8ae3-08de427e8188\",\n",
      "  \"runId\" : \"fb597df6-a86d-4639-a419-eb22ee826f40\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-02T17:35:40.000Z\",\n",
      "  \"batchId\" : 8,\n",
      "  \"numInputRows\" : 5,\n",
      "  \"inputRowsPerSecond\" : 0.5000500050005,\n",
      "  \"processedRowsPerSecond\" : 10.615711252653929,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 313,\n",
      "    \"commitOffsets\" : 30,\n",
      "    \"getBatch\" : 44,\n",
      "    \"latestOffset\" : 37,\n",
      "    \"queryPlanning\" : 14,\n",
      "    \"triggerExecution\" : 471,\n",
      "    \"walCommit\" : 32\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/structured_streaming_files/logs]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 7\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 8\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 5,\n",
      "    \"inputRowsPerSecond\" : 0.5000500050005,\n",
      "    \"processedRowsPerSecond\" : 10.615711252653929\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3b4b7fb5\",\n",
      "    \"numOutputRows\" : 2\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "|                 log|          timestamp|level|      message|       server|\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "|[2025-04-02 17:34...|2025-04-02 17:34:46|ERROR|404 Not Found|server-node-2|\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "|                 log|          timestamp|level|      message|       server|\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "|[2025-04-02 17:35...|2025-04-02 17:35:09|ERROR|404 Not Found|server-node-2|\n",
      "|[2025-04-02 17:35...|2025-04-02 17:35:11|ERROR|404 Not Found|server-node-2|\n",
      "+--------------------+-------------------+-----+-------------+-------------+\n",
      "\n",
      "Query made progress: {\n",
      "  \"id\" : \"1ce9aab7-f610-4f85-b7fe-ce08ff5d96c1\",\n",
      "  \"runId\" : \"2dafe564-a7c6-499f-bfa7-356846739c45\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-02T17:35:50.000Z\",\n",
      "  \"batchId\" : 4,\n",
      "  \"numInputRows\" : 5,\n",
      "  \"inputRowsPerSecond\" : 0.5000500050005,\n",
      "  \"processedRowsPerSecond\" : 3.885003885003885,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 1000,\n",
      "    \"commitOffsets\" : 28,\n",
      "    \"getBatch\" : 78,\n",
      "    \"latestOffset\" : 30,\n",
      "    \"queryPlanning\" : 113,\n",
      "    \"triggerExecution\" : 1287,\n",
      "    \"walCommit\" : 27\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/structured_streaming_files/logs]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 3\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 4\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 5,\n",
      "    \"inputRowsPerSecond\" : 0.5000500050005,\n",
      "    \"processedRowsPerSecond\" : 3.885003885003885\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3b4b7fb5\",\n",
      "    \"numOutputRows\" : 1\n",
      "  }\n",
      "}\n",
      "Query made progress: {\n",
      "  \"id\" : \"9b58389b-0b9e-46ad-8ae3-08de427e8188\",\n",
      "  \"runId\" : \"fb597df6-a86d-4639-a419-eb22ee826f40\",\n",
      "  \"name\" : null,\n",
      "  \"timestamp\" : \"2025-04-02T17:35:50.000Z\",\n",
      "  \"batchId\" : 9,\n",
      "  \"numInputRows\" : 5,\n",
      "  \"inputRowsPerSecond\" : 0.5,\n",
      "  \"processedRowsPerSecond\" : 2.6191723415400734,\n",
      "  \"durationMs\" : {\n",
      "    \"addBatch\" : 1322,\n",
      "    \"commitOffsets\" : 231,\n",
      "    \"getBatch\" : 162,\n",
      "    \"latestOffset\" : 47,\n",
      "    \"queryPlanning\" : 113,\n",
      "    \"triggerExecution\" : 1909,\n",
      "    \"walCommit\" : 28\n",
      "  },\n",
      "  \"stateOperators\" : [ ],\n",
      "  \"sources\" : [ {\n",
      "    \"description\" : \"FileStreamSource[file:/home/jovyan/notebooks/data/structured_streaming_files/logs]\",\n",
      "    \"startOffset\" : {\n",
      "      \"logOffset\" : 8\n",
      "    },\n",
      "    \"endOffset\" : {\n",
      "      \"logOffset\" : 9\n",
      "    },\n",
      "    \"latestOffset\" : null,\n",
      "    \"numInputRows\" : 5,\n",
      "    \"inputRowsPerSecond\" : 0.5,\n",
      "    \"processedRowsPerSecond\" : 2.6191723415400734\n",
      "  } ],\n",
      "  \"sink\" : {\n",
      "    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@3b4b7fb5\",\n",
      "    \"numOutputRows\" : 2\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = structured_logs_df \\\n",
    "                .writeStream \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .trigger(processingTime='10 seconds') \\\n",
    "                .format(\"console\") \\\n",
    "                .start()\n",
    "\n",
    "query.awaitTermination(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
