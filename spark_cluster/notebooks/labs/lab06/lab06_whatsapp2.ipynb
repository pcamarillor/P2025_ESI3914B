{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Ejemplos de Spark SQL: Soluciones de almacenamiento para Big Data** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/07 13:20:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL-Storage-Solutions\") \\\n",
    "    .master(\"spark://56a250e0d184:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Car rental service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- agency_id: string (nullable = true)\n",
      " |-- agency_info: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------------------------+\n",
      "|agency_id|agency_info                                          |\n",
      "+---------+-----------------------------------------------------+\n",
      "|1        |{'agency_name': 'NYC Rentals', 'city': 'New York'}   |\n",
      "|2        |{'agency_name': 'LA Car Rental', 'city': 'Londres'}  |\n",
      "|3        |{'agency_name': 'Zapopan Auto', 'city': 'Zapopan'}   |\n",
      "|4        |{'agency_name': 'SF Cars', 'city': 'San Francisco'}  |\n",
      "|5        |{'agency_name': 'Mexico Cars', 'city': 'Mexico City'}|\n",
      "+---------+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from team_name.spark_utils import SparkUtils\n",
    "agencies_schema = SparkUtils.generate_schema([(\"agency_id\", \"string\"), (\"agency_info\", \"string\")])\n",
    "\n",
    "agencies_df = spark.read \\\n",
    "                .schema(agencies_schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/jovyan/notebooks/data/rentals_dataset/agencies.csv\")\n",
    "\n",
    "agencies_df.printSchema()\n",
    "\n",
    "agencies_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------------------------+-------------+\n",
      "|agency_id|agency_info                                          |agency_name  |\n",
      "+---------+-----------------------------------------------------+-------------+\n",
      "|1        |{'agency_name': 'NYC Rentals', 'city': 'New York'}   |NYC Rentals  |\n",
      "|2        |{'agency_name': 'LA Car Rental', 'city': 'Londres'}  |LA Car Rental|\n",
      "|3        |{'agency_name': 'Zapopan Auto', 'city': 'Zapopan'}   |Zapopan Auto |\n",
      "|4        |{'agency_name': 'SF Cars', 'city': 'San Francisco'}  |SF Cars      |\n",
      "|5        |{'agency_name': 'Mexico Cars', 'city': 'Mexico City'}|Mexico Cars  |\n",
      "+---------+-----------------------------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "agencies_df = agencies_df.withColumn(\"agency_name\", get_json_object(agencies_df.agency_info, \"$.agency_name\"))\n",
    "agencies_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand_id: integer (nullable = true)\n",
      " |-- brand_info: string (nullable = true)\n",
      "\n",
      "+--------+------------------------------------------------------+-------------+\n",
      "|brand_id|brand_info                                            |brand_name   |\n",
      "+--------+------------------------------------------------------+-------------+\n",
      "|1       |{'brand_name': 'Mercedes-Benz', 'country': 'Tanzania'}|Mercedes-Benz|\n",
      "|2       |{'brand_name': 'BMW', 'country': 'Hungary'}           |BMW          |\n",
      "|3       |{'brand_name': 'Audi', 'country': 'Senegal'}          |Audi         |\n",
      "|4       |{'brand_name': 'Ford', 'country': 'Tuvalu'}           |Ford         |\n",
      "|5       |{'brand_name': 'BYD', 'country': 'Italy'}             |BYD          |\n",
      "+--------+------------------------------------------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brands_schema = SparkUtils.generate_schema([(\"brand_id\", \"integer\"), (\"brand_info\", \"string\")])\n",
    "brands_df = spark.read.option(\"header\", \"true\").schema(brands_schema).csv(\"/home/jovyan/notebooks/data/rentals_dataset/brands.csv\")\n",
    "brands_df.printSchema()\n",
    "brands_df = brands_df.withColumn(\"brand_name\", get_json_object(brands_df.brand_info, \"$.brand_name\"))\n",
    "brands_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- car_id: integer (nullable = true)\n",
      " |-- car_info: string (nullable = true)\n",
      "\n",
      "+------+-------------------------------------------------------------------------------------+---------------------------------+--------+\n",
      "|car_id|car_info                                                                             |car_name                         |brand_id|\n",
      "+------+-------------------------------------------------------------------------------------+---------------------------------+--------+\n",
      "|1     |{'car_name': 'Tucker, Hull and Gallegos Model 1', 'brand_id': 5, 'price_per_day': 68}|Tucker, Hull and Gallegos Model 1|5       |\n",
      "|2     |{'car_name': 'Howard-Snow Model 7', 'brand_id': 5, 'price_per_day': 55}              |Howard-Snow Model 7              |5       |\n",
      "|3     |{'car_name': 'Wagner LLC Model 2', 'brand_id': 2, 'price_per_day': 194}              |Wagner LLC Model 2               |2       |\n",
      "|4     |{'car_name': 'Campos PLC Model 8', 'brand_id': 1, 'price_per_day': 107}              |Campos PLC Model 8               |1       |\n",
      "|5     |{'car_name': 'Archer-Patel Model 3', 'brand_id': 4, 'price_per_day': 136}            |Archer-Patel Model 3             |4       |\n",
      "+------+-------------------------------------------------------------------------------------+---------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_schema = SparkUtils.generate_schema([(\"car_id\", \"integer\"), (\"car_info\", \"string\")])\n",
    "cars_df = spark.read.option(\"header\", \"true\").schema(cars_schema).csv(\"/home/jovyan/notebooks/data/rentals_dataset/cars.csv\")\n",
    "cars_df.printSchema()\n",
    "cars_df = cars_df.withColumn(\"car_name\", get_json_object(cars_df.car_info, \"$.car_name\")) \\\n",
    "                .withColumn(\"brand_id\", get_json_object(cars_df.car_info, \"$.brand_id\")) \n",
    "cars_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_info: string (nullable = true)\n",
      "\n",
      "+-----------+--------------------+-------------------+\n",
      "|customer_id|       customer_info|      customer_name|\n",
      "+-----------+--------------------+-------------------+\n",
      "|          1|{'customer_name':...|  Martin Graves DVM|\n",
      "|          2|{'customer_name':...|   Frederick Wilson|\n",
      "|          3|{'customer_name':...|       Gabriela Lee|\n",
      "|          4|{'customer_name':...|     Devin Thornton|\n",
      "|          5|{'customer_name':...|Christopher Simmons|\n",
      "+-----------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customers_schema = SparkUtils.generate_schema([(\"customer_id\", \"integer\"), (\"customer_info\", \"string\")])\n",
    "customers_df = spark.read.option(\"header\", \"true\").schema(customers_schema).csv(\"/home/jovyan/notebooks/data/rentals_dataset/customers.csv\")\n",
    "customers_df.printSchema()\n",
    "customers_df = customers_df.withColumn(\"customer_name\", get_json_object(customers_df.customer_info, \"$.customer_name\"))\n",
    "customers_df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- rental_info: string (nullable = true)\n",
      "\n",
      "+---------+--------------------------------------------------+\n",
      "|rental_id|rental_info                                       |\n",
      "+---------+--------------------------------------------------+\n",
      "|12740    |{'car_id': 23, 'customer_id': 42, 'agency_id': 1} |\n",
      "|12741    |{'car_id': 19, 'customer_id': 146, 'agency_id': 2}|\n",
      "|12742    |{'car_id': 24, 'customer_id': 143, 'agency_id': 3}|\n",
      "|12743    |{'car_id': 22, 'customer_id': 90, 'agency_id': 4} |\n",
      "|12744    |{'car_id': 9, 'customer_id': 115, 'agency_id': 3} |\n",
      "+---------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rental_cars_schema = SparkUtils.generate_schema([(\"rental_id\", \"integer\"), (\"rental_info\", \"string\")])\n",
    "rental_cars_df = spark.read.option(\"header\", \"true\").schema(rental_cars_schema).csv(\"/home/jovyan/notebooks/data/rentals_dataset/rentals/\")\n",
    "rental_cars_df.printSchema()\n",
    "rental_cars_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------------------------+------+-----------+---------+\n",
      "|rental_id|rental_info                                       |car_id|customer_id|agency_id|\n",
      "+---------+--------------------------------------------------+------+-----------+---------+\n",
      "|12740    |{'car_id': 23, 'customer_id': 42, 'agency_id': 1} |23    |42         |1        |\n",
      "|12741    |{'car_id': 19, 'customer_id': 146, 'agency_id': 2}|19    |146        |2        |\n",
      "|12742    |{'car_id': 24, 'customer_id': 143, 'agency_id': 3}|24    |143        |3        |\n",
      "|12743    |{'car_id': 22, 'customer_id': 90, 'agency_id': 4} |22    |90         |4        |\n",
      "|12744    |{'car_id': 9, 'customer_id': 115, 'agency_id': 3} |9     |115        |3        |\n",
      "+---------+--------------------------------------------------+------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rental_cars_df = rental_cars_df.withColumn(\"car_id\", get_json_object(rental_cars_df.rental_info, '$.car_id')) \\\n",
    "                            .withColumn(\"customer_id\", get_json_object(rental_cars_df.rental_info, '$.customer_id')) \\\n",
    "                            .withColumn(\"agency_id\", get_json_object(rental_cars_df.rental_info, '$.agency_id'))\n",
    "\n",
    "rental_cars_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------------------------------------+------+-----------+---------+------+-----------------------------------------------------------------------------------+-------------------------------+--------+---------+---------------------------------------------------+-------------+-----------+------------------------------------------------------------------------+---------------+\n",
      "|rental_id|rental_info                                       |car_id|customer_id|agency_id|car_id|car_info                                                                           |car_name                       |brand_id|agency_id|agency_info                                        |agency_name  |customer_id|customer_info                                                           |customer_name  |\n",
      "+---------+--------------------------------------------------+------+-----------+---------+------+-----------------------------------------------------------------------------------+-------------------------------+--------+---------+---------------------------------------------------+-------------+-----------+------------------------------------------------------------------------+---------------+\n",
      "|12740    |{'car_id': 23, 'customer_id': 42, 'agency_id': 1} |23    |42         |1        |23    |{'car_name': 'Salazar Ltd Model 6', 'brand_id': 6, 'price_per_day': 163}           |Salazar Ltd Model 6            |6       |1        |{'agency_name': 'NYC Rentals', 'city': 'New York'} |NYC Rentals  |42         |{'customer_name': 'Sara Anderson', 'city': 'Monterrey', 'age': 30}      |Sara Anderson  |\n",
      "|12741    |{'car_id': 19, 'customer_id': 146, 'agency_id': 2}|19    |146        |2        |19    |{'car_name': 'Harris, Lloyd and Payne Model 4', 'brand_id': 2, 'price_per_day': 91}|Harris, Lloyd and Payne Model 4|2       |2        |{'agency_name': 'LA Car Rental', 'city': 'Londres'}|LA Car Rental|146        |{'customer_name': 'Calvin Walker', 'city': 'Mexico City', 'age': 29}    |Calvin Walker  |\n",
      "|12742    |{'car_id': 24, 'customer_id': 143, 'agency_id': 3}|24    |143        |3        |24    |{'car_name': 'Alvarez-Davis Model 6', 'brand_id': 7, 'price_per_day': 104}         |Alvarez-Davis Model 6          |7       |3        |{'agency_name': 'Zapopan Auto', 'city': 'Zapopan'} |Zapopan Auto |143        |{'customer_name': 'Shawn Tran', 'city': 'Monterrey', 'age': 24}         |Shawn Tran     |\n",
      "|12743    |{'car_id': 22, 'customer_id': 90, 'agency_id': 4} |22    |90         |4        |22    |{'car_name': 'Lopez and Sons Model 3', 'brand_id': 2, 'price_per_day': 141}        |Lopez and Sons Model 3         |2       |4        |{'agency_name': 'SF Cars', 'city': 'San Francisco'}|SF Cars      |90         |{'customer_name': 'Edward Mccarthy', 'city': 'San Francisco', 'age': 48}|Edward Mccarthy|\n",
      "|12744    |{'car_id': 9, 'customer_id': 115, 'agency_id': 3} |9     |115        |3        |9     |{'car_name': 'Levy Group Model 8', 'brand_id': 1, 'price_per_day': 57}             |Levy Group Model 8             |1       |3        |{'agency_name': 'Zapopan Auto', 'city': 'Zapopan'} |Zapopan Auto |115        |{'customer_name': 'Antonio Haynes', 'city': 'Mexico City', 'age': 25}   |Antonio Haynes |\n",
      "+---------+--------------------------------------------------+------+-----------+---------+------+-----------------------------------------------------------------------------------+-------------------------------+--------+---------+---------------------------------------------------+-------------+-----------+------------------------------------------------------------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rental_cars_df = rental_cars_df.join(cars_df, rental_cars_df.car_id == cars_df.car_id, \"inner\") \\\n",
    "                                .join(agencies_df, rental_cars_df.agency_id == agencies_df.agency_id, \"inner\") \\\n",
    "                                .join(customers_df, rental_cars_df.customer_id == customers_df.customer_id, \"inner\")\n",
    "\n",
    "rental_cars_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final result\n",
    "rental_cars_df = rental_cars_df.select(\"rental_id\", \"car_name\", \"agency_name\", \"customer_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------------+-------------+---------------+\n",
      "|rental_id|car_name                       |agency_name  |customer_name  |\n",
      "+---------+-------------------------------+-------------+---------------+\n",
      "|12740    |Salazar Ltd Model 6            |NYC Rentals  |Sara Anderson  |\n",
      "|12741    |Harris, Lloyd and Payne Model 4|LA Car Rental|Calvin Walker  |\n",
      "|12742    |Alvarez-Davis Model 6          |Zapopan Auto |Shawn Tran     |\n",
      "|12743    |Lopez and Sons Model 3         |SF Cars      |Edward Mccarthy|\n",
      "|12744    |Levy Group Model 8             |Zapopan Auto |Antonio Haynes |\n",
      "+---------+-------------------------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rental_cars_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_cars_df.createOrReplaceTempView(\"rentals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|rental_id|  customer_name|\n",
      "+---------+---------------+\n",
      "|    12740|  Sara Anderson|\n",
      "|    12741|  Calvin Walker|\n",
      "|    12742|     Shawn Tran|\n",
      "|    12743|Edward Mccarthy|\n",
      "|    12744| Antonio Haynes|\n",
      "+---------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT rental_id, customer_name FROM rentals\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|  agency_name|rentals_count|\n",
      "+-------------+-------------+\n",
      "| Zapopan Auto|         4425|\n",
      "|LA Car Rental|         4451|\n",
      "|      SF Cars|         4481|\n",
      "|  NYC Rentals|         4477|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT agency_name, count(*) as rentals_count FROM rentals GROUP BY agency_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register temporal views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agencies_df.createOrReplaceTempView(\"agencies\")\n",
    "brands_df.createOrReplaceTempView(\"brands\")\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "rental_cars_df.createOrReplaceTempView(\"rentals\")\n",
    "cars_df.createOrReplaceTempView(\"cars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the most popular car brands by the number of rentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------+\n",
      "|car_id|            car_name|   brand_name|\n",
      "+------+--------------------+-------------+\n",
      "|     1|Tucker, Hull and ...|          BYD|\n",
      "|     2| Howard-Snow Model 7|          BYD|\n",
      "|     3|  Wagner LLC Model 2|          BMW|\n",
      "|     4|  Campos PLC Model 8|Mercedes-Benz|\n",
      "|     5|Archer-Patel Model 3|         Ford|\n",
      "+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re create cars view\n",
    "cars_df = spark.read.option(\"header\", \"true\").schema(cars_schema).csv(\"/home/jovyan/notebooks/data/rentals_dataset/cars.csv\")\n",
    "cars_df.createOrReplaceTempView(\"cars\")\n",
    "cars_df = spark.sql(\"\"\"\n",
    "                    SELECT c.car_id, \n",
    "                           get_json_object(c.car_info, '$.car_name') AS car_name,\n",
    "                           get_json_object(b.brand_info, '$.brand_name') AS brand_name\n",
    "                    FROM cars c \n",
    "                    JOIN brands b\n",
    "                    ON get_json_object(c.car_info, '$.brand_id') = b.brand_id\n",
    "                    \"\"\")\n",
    "cars_df.createOrReplaceTempView(\"cars\")\n",
    "spark.sql(\"SELECT * FROM cars\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|brand_name|rental_count|\n",
      "+----------+------------+\n",
      "|       BMW|        5471|\n",
      "|     Honda|        4258|\n",
      "|      Ford|        2473|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re create rentals view\n",
    "rental_cars_df = spark.read.option(\"header\", \"true\").schema(rental_cars_schema).csv(\"/home/jovyan/notebooks/data/rentals_dataset/rentals/\")\n",
    "rental_cars_df.createOrReplaceTempView(\"rentals\")\n",
    "rental_cars_df = spark.sql(\"\"\"\n",
    "            SELECT r.rental_id,\n",
    "                   c.car_name,\n",
    "                   c.brand_name,\n",
    "                   a.agency_name,\n",
    "                   cus.customer_name\n",
    "            FROM rentals r\n",
    "            JOIN cars c ON get_json_object(r.rental_info, '$.car_id') = c.car_id\n",
    "            JOIN agencies a ON get_json_object(r.rental_info, '$.agency_id') = a.agency_id\n",
    "            JOIN customers cus ON get_json_object(r.rental_info, '$.customer_id') = cus.customer_id\n",
    "          \"\"\")\n",
    "rental_cars_df.createOrReplaceTempView(\"rentals\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          SELECT brand_name, count(*) AS rental_count FROM rentals \n",
    "          GROUP BY brand_name ORDER BY rental_count DESC LIMIT 3\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the top 5 customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|    customer_name|rental_count|\n",
      "+-----------------+------------+\n",
      "|Catherine Alvarez|         136|\n",
      "|    Travis Butler|         133|\n",
      "|     Corey Wilson|         132|\n",
      "|      Ronald Hall|         131|\n",
      "|    Cynthia White|         131|\n",
      "+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT customer_name, count(*) AS rental_count FROM rentals GROUP BY customer_name ORDER BY rental_count DESC LIMIT 5\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find which car generate the most revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            car_name|rental_count|\n",
      "+--------------------+------------+\n",
      "|Bryan, Barnes and...|         654|\n",
      "|Clayton-Cook Mode...|         653|\n",
      "|Summers, Barnett ...|         653|\n",
      "|Harris, Lloyd and...|         649|\n",
      "|Myers, Thornton a...|         643|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT car_name, count(*) AS rental_count FROM rentals GROUP BY car_name ORDER BY rental_count DESC LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rental_cars_df.write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .partitionBy(\"agency_name\") \\\n",
    "                .parquet(\"/home/jovyan/notebooks/data/rentals_output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SparkUtils.clean_df() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/notebooks/data/netflix2.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Limpiar y guardar CSV\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mSparkUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/notebooks/data/oaxaca.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/jovyan/notebooks/data/oaxaca_clean.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Escribir en formato Parquet con partición\u001b[39;00m\n\u001b[1;32m     12\u001b[0m SparkUtils\u001b[38;5;241m.\u001b[39mwrite_df(spark, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/notebooks/data/oaxaca_clean.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jovyan/notebooks/data/oaxacaclean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelease_year\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: SparkUtils.clean_df() takes 1 positional argument but 3 were given"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/07 15:13:53 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-d3d21da4-ab82-4e19-b2d9-3276fce40e29. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-d3d21da4-ab82-4e19-b2d9-3276fce40e29\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:210)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/03/07 15:13:53 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-d3d21da4-ab82-4e19-b2d9-3276fce40e29/13. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-d3d21da4-ab82-4e19-b2d9-3276fce40e29/13\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:210)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from whatsapp2.spark_utils import SparkUtils\n",
    "# Load the CSV file\n",
    "path = \"/home/jovyan/notebooks/data/netflix1.csv\"\n",
    "output_path = \"/home/jovyan/notebooks/data/netflix2.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Limpiar y guardar CSV\n",
    "SparkUtils.clean_df(spark, \"/home/jovyan/notebooks/data/oaxaca.csv\", \"/home/jovyan/notebooks/data/oaxaca_clean.csv\")\n",
    "\n",
    "# Escribir en formato Parquet con partición\n",
    "SparkUtils.write_df(spark, \"/home/jovyan/notebooks/data/oaxaca_clean.csv\", \"/home/jovyan/notebooks/data/oaxacaclean\", \"release_year\")\n",
    "\n",
    "\n",
    "\n",
    "SparkUtils.clean_df()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "df = spark.read.option(\"header\", True).csv(\"/home/jovyan/notebooks/data/oaxaca.csv\")\n",
    "df_cleaned = df.dropna()\n",
    "output_path = \"/home/jovyan/notebooks/data/oaxacaclean\"\n",
    "df_cleaned.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"release_year\") \\\n",
    "    .parquet(output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkContext\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
