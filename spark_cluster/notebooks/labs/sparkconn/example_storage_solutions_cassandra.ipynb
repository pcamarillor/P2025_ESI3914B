{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Ejemplos de Spark SQL: Soluciones de almacenamiento para Big Data (Postgres)** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f661edfc-3ce5-4a5d-ac16-367991f9f3ad;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.13;3.5.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.13;3.5.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.13;2.11.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.13.11 in central\n",
      ":: resolution report :: resolve 947ms :: artifacts dl 49ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.13;3.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.13;3.5.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.13.11 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.13;2.11.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   20  |   0   |   0   |   0   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f661edfc-3ce5-4a5d-ac16-367991f9f3ad\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/21ms)\n",
      "25/03/21 13:16:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL-Storage-Solutions-Cassandra-JJAE\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"cassandra-iteso\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.13:3.5.0\") \\\n",
    "    .config(\"spark.ui.port\", \"4040\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- show_id: string (nullable = false)\n",
      " |-- country: string (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- listed_in: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- release_year: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------+--------+---------+------+------------+-----+----+\n",
      "|show_id|country|date_added|director|duration|listed_in|rating|release_year|title|type|\n",
      "+-------+-------+----------+--------+--------+---------+------+------------+-----+----+\n",
      "+-------+-------+----------+--------+--------+---------+------+------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = spark.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(table=\"netflix\", keyspace=\"my_keyspace\") \\\n",
    "        .load()\n",
    "    df.printSchema()\n",
    "    df.show()  # Should show empty result if the table has no data, but without crashing\n",
    "except Exception as e:\n",
    "    print(f\"Connection or read error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.4\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos el DF de Netflix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- show_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- release_year: integer (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- listed_in: string (nullable = true)\n",
      "\n",
      "+-------+-------+--------------------------------+---------------+-------------+----------+------------+------+--------+-------------------------------------------------------------+\n",
      "|show_id|type   |title                           |director       |country      |date_added|release_year|rating|duration|listed_in                                                    |\n",
      "+-------+-------+--------------------------------+---------------+-------------+----------+------------+------+--------+-------------------------------------------------------------+\n",
      "|s1     |Movie  |Dick Johnson Is Dead            |Kirsten Johnson|United States|9/25/2021 |2020        |PG-13 |90 min  |Documentaries                                                |\n",
      "|s3     |TV Show|Ganglands                       |Julien Leclercq|France       |9/24/2021 |2021        |TV-MA |1 Season|Crime TV Shows, International TV Shows, TV Action & Adventure|\n",
      "|s6     |TV Show|Midnight Mass                   |Mike Flanagan  |United States|9/24/2021 |2021        |TV-MA |1 Season|TV Dramas, TV Horror, TV Mysteries                           |\n",
      "|s14    |Movie  |Confessions of an Invisible Girl|Bruno Garotti  |Brazil       |9/22/2021 |2021        |TV-PG |91 min  |Children & Family Movies, Comedies                           |\n",
      "|s8     |Movie  |Sankofa                         |Haile Gerima   |United States|9/24/2021 |1993        |TV-MA |125 min |Dramas, Independent Movies, International Movies             |\n",
      "+-------+-------+--------------------------------+---------------+-------------+----------+------------+------+--------+-------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from team_name.spark_utils import SparkUtils\n",
    "\n",
    "columns_info = [(\"show_id\", \"string\"),(\"type\", \"string\"),(\"title\", \"string\"),(\"director\", \"string\"),(\"country\", \"string\"),(\"date_added\", \"string\"),(\"release_year\", \"integer\"),(\"rating\", \"string\"), (\"duration\", \"string\"),(\"listed_in\", \"string\")]\n",
    "\n",
    "netflix_schema = SparkUtils.generate_schema(columns_info)\n",
    "\n",
    "netflix_df = spark.read \\\n",
    "                .schema(netflix_schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/jovyan/notebooks/data/netflix1.csv\")\n",
    "\n",
    "netflix_df.printSchema()\n",
    "\n",
    "netflix_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escribir un DF a Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "netflix_df.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .options(table=\"netflix\", keyspace=\"my_keyspace\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leer info desde Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----------+--------------------+--------+--------------------+------+------------+---------------+-----+\n",
      "|show_id|       country|date_added|            director|duration|           listed_in|rating|release_year|          title| type|\n",
      "+-------+--------------+----------+--------------------+--------+--------------------+------+------------+---------------+-----+\n",
      "|  s6541| United States| 10/1/2019|         Paul Haggis| 112 min|Dramas, Independe...|     R|        2004|          Crash|Movie|\n",
      "|  s7516| United States|  4/9/2019|Peter Farrelly, W...|  94 min|            Comedies|     R|        2013|       Movie 43|Movie|\n",
      "|   s586|United Kingdom|  7/1/2021|     Claire McCarthy| 106 min|Dramas, Romantic ...| PG-13|        2018|        Ophelia|Movie|\n",
      "|  s8265|         India|  1/1/2018|       Satish Manwar|  99 min|Dramas, Internati...| TV-14|        2009|The Damned Rain|Movie|\n",
      "|  s7834|      Pakistan|  8/1/2018|      Amir Mohiuddin| 126 min|Dramas, Internati...| TV-14|        2017|       Rangreza|Movie|\n",
      "+-------+--------------+----------+--------------------+--------+--------------------+------+------------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/21 14:40:23 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-a5655a2d-d23a-4df8-a4d6-383dc7e294ff. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-a5655a2d-d23a-4df8-a4d6-383dc7e294ff\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:210)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "# Read from Cassandra\n",
    "cassandra_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .option(\"keyspace\", \"my_keyspace\") \\\n",
    "    .option(\"table\", \"netflix\").load() \n",
    "\n",
    "cassandra_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/21 05:05:19 WARN ChannelPool: [s0|cassandra-iteso/172.18.0.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=8f58954a-3fb6-4df6-942f-7fad6939c982, APPLICATION_NAME=Spark-Cassandra-Connector-app-20250321043324-0000}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/03/21 05:05:19 WARN ControlConnection: [s0] Error connecting to Node(endPoint=cassandra-iteso/172.18.0.2:9042, hostId=703cfdb6-e72b-4e40-957c-73c00437f99d, hashCode=35fb762c), trying next node (ConnectionInitException: [s0|control|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=8f58954a-3fb6-4df6-942f-7fad6939c982, APPLICATION_NAME=Spark-Cassandra-Connector-app-20250321043324-0000}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/03/21 05:05:21 WARN ChannelPool: [s0|cassandra-iteso/172.18.0.2:9042]  Error while opening new channel (ConnectionInitException: [s0|connecting...] Protocol initialization request, step 1 (STARTUP {CQL_VERSION=3.0.0, DRIVER_NAME=DataStax Java driver for Apache Cassandra(R), DRIVER_VERSION=4.13.0, CLIENT_ID=8f58954a-3fb6-4df6-942f-7fad6939c982, APPLICATION_NAME=Spark-Cassandra-Connector-app-20250321043324-0000}): failed to send request (com.datastax.oss.driver.shaded.netty.channel.StacklessClosedChannelException))\n",
      "25/03/21 05:05:22 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-2d09193b-288a-4ab4-b7cd-fc14b470da31. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-2d09193b-288a-4ab4-b7cd-fc14b470da31\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:210)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
